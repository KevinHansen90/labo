frequency(nottem)
summary(nottem)
nottem[1,:]
nottem[1,1]
start(nottem)
end(nottem)
first(nottem)
nottem
nottem[1]
nottem[-1]
nottem[:-1]
tail(nottem)
tail(nottem,1)
head(nottem,1)
plot(nottem, col = "magenta")
boxplot(nottem~cycle(nottem),col ="blue")
cycle(nottem)
plot(decompose(nottem))
AP<- ts(nottem,frequency = 12, start = c(1920,1), end = c(1938,12))
modelo1 <- auto.arima(AP, stationary =FALSE, seasonal = TRUE)
summary(modelo1)
modelo1 <- auto.arima(AP, stationary =FALSE, seasonal = TRUE)
install.packages("forecast")
modelo1 <- auto.arima(AP, stationary =FALSE, seasonal = TRUE)
summary(modelo1)
library(forecast)
modelo1 <- auto.arima(AP, stationary =FALSE, seasonal = TRUE)
summary(modelo1)
ajuste<- forecast(modelo1, h = 12)
ajuste
ajuste$mean
plot(ajuste)
#Error del pronóstico
SF<- window(nottem,start=c(1939,1),end=c(1939,12))
SF
SFAjuste = cbind(SF,ajuste$mean)
SFAjuste
(1/12)* sum((ajuste$mean - SF)^2)
ggplot(nottem, aes(y = nottem, x=time(nottem)))+geom_line()+geom_smooth(se=FALSE)
library(ggplot2)
SF<- window(nottem,start=c(1939,1),end=c(1939,12))
SF
SFAjuste = cbind(SF,ajuste$mean)
SFAjuste
(1/12)* sum((ajuste$mean - SF)^2)
ggplot(nottem, aes(y = nottem, x=time(nottem)))+geom_line()+geom_smooth(se=FALSE)
ajuste$mean
plot(ajuste)
ajuste
ajuste$mean
ajuste
ajuste$mean
(1/12)* sum((ajuste$mean - SF)^2)
load(""C:\\Users\\HP\\Downloads\\acath.sav"")
load("C:\\Users\\HP\\Downloads\\acath.sav")
load("C:\\Users\\HP\\Downloads\\acath.sav")
acath
dim(acath)
head(acath)
summary(acath)
data_space <- ggplot(data = acath, aes(y = sigdz, x = choleste)) +
geom_jitter(width = 0, height = 0.05, alpha = 0.5)
library(ggplot2)
data_space <- ggplot(data = acath, aes(y = sigdz, x = choleste)) +
geom_jitter(width = 0, height = 0.05, alpha = 0.5)
data_space
data_space +
geom_smooth(method = "glm",method.args = list(family = "binomial"))
ggplot(data = acath, aes(y = sigdz, x = choleste)) +
geom_jitter(width = 0, height = 0.05, alpha = 0.5)
logit1<- glm(sigdz ~ choleste, data = acath, family = "binomial")
summary(logit1)
data_space +
geom_smooth(method = "glm",method.args = list(family = "binomial"))
data_space +
geom_smooth(method = "glm",method.args = list(family = "binomial"))
data_space <- ggplot(data = acath, aes(y = sigdz, x = choleste)) +
geom_jitter(width = 0, height = 0.05, alpha = 0.5, main="Regresión Logística: sigdz vs. choleste")
# linear regression line
data_space +
geom_smooth(method = "glm",method.args = list(family = "binomial"))
data_space +
geom_smooth(method = "glm",method.args = list(family = "binomial",main="Regresión Logística: sigdz vs. choleste"))
data_space +
geom_smooth(method = "glm",method.args = list(family = "binomial"))+main="Regresión Logística: sigdz vs. choleste"
ggplot(data = acath, aes(y = sigdz, x = choleste), main="Regresión Logística: sigdz vs. choleste")
+geom_jitter(width = 0, height = 0.05, alpha = 0.5)
+geom_smooth(method = "glm",method.args = list(family = "binomial"))
ggplot(data = acath, aes(y = sigdz, x = choleste), title="Regresión Logística: sigdz vs. choleste")
+geom_jitter(width = 0, height = 0.05, alpha = 0.5)
+geom_smooth(method = "glm",method.args = list(family = "binomial"))
ggplot(data = acath, aes(y = sigdz, x = choleste))
+geom_jitter(width = 0, height = 0.05, alpha = 0.5)
+geom_smooth(method = "glm",method.args = list(family = "binomial"))
ggplot(data = acath, aes(y = sigdz, x = choleste))
+geom_jitter(width = 0, height = 0.05, alpha = 0.5)
ggplot(data = acath, aes(y = sigdz, x = choleste))+geom_jitter(width = 0, height = 0.05, alpha = 0.5)+geom_smooth(method = "glm",method.args = list(family = "binomial"))
ggplot(data = acath, aes(y = sigdz, x = choleste),title="Regresión Logística: sigdz vs. choleste")+geom_jitter(width = 0, height = 0.05, alpha = 0.5)+geom_smooth(method = "glm",method.args = list(family = "binomial"))
ggplot(data = acath, aes(y = sigdz, x = choleste))+geom_jitter(width = 0, height = 0.05, alpha = 0.5)+geom_smooth(method = "glm",method.args = list(family = "binomial"))+ggtitle("Regresión Logística: sigdz vs. choleste")
summary(logit1)
ggplot(data = acath, aes(y = sigdz, x = choleste))+geom_jitter(width = 0, height = 0.05, alpha = 0.5)+geom_smooth(method = "glm",method.args = list(family = "binomial"))+ggtitle("Regresión Logística: sigdz vs. choleste")
ggplot(data = acath, aes(y = sigdz, x = choleste))+geom_jitter(width = 0, height = 0.05, alpha = 0.5)+geom_smooth(method = "glm",method.args = list(family = "binomial"))+ggtitle("Regresión Logística: sigdz vs. choleste")
ggplot(data = acath, aes(y = sigdz, x = choleste))+geom_jitter(width = 0, height = 0.05, alpha = 0.5)+geom_smooth(method = "glm",method.args = list(family = "binomial"))+ggtitle("Regresión Logística: sigdz vs. choleste")
logit1<- glm(sigdz ~ choleste, data = acath, family = "binomial")
summary(logit1)
logit1<- glm(sigdz ~ choleste, data = acath, family = "binomial")
summary(logit1)
newdata = data.frame(choleste=c(199))
prediccion <- predict(logit1, newdata, type = 'response')
prediccion
logit2<- glm(sigdz ~ choleste+age+cad.dur, data = acath, family = "binomial")
summary(logit2)
logit2<- glm(sigdz ~ choleste+age+cad.dur, data = acath, family = "binomial")
summary(logit2)
View(acath)
View(acath)
logit3<- glm(sigdz ~ choleste+age+cad.dur+sex, data = acath, family = "binomial")
summary(logit3)
Caseros= c(11, 14, 7, 15, 11, 13, 11, 16, 10, 15, 18, 12, 9, 9, 10, 10, 15, 10, 14, 10, 10, 12, 14, 12, 15, 7, 13, 6,
10, 15 ,20, 10 ,13 ,10, 6 ,14, 8, 10, 8, 11)
SantosLugares= c(13, 10, 12, 7, 5, 10, 10, 16, 9, 7, 7, 2, 6, 9, 9, 8, 8, 10, 3, 6, 5, 2, 9, 3, 4, 5, 10, 8, 5, 9, 10, 8,
13, 10, 0, 2, 1, 1, 0, 4)
PabloPodesta= c(6, 7, 3, 5, 9, 6, 1, 6, 0, 2, 5, 6, 11, 6, 7, 0, 5, 7, 5, 4, 7, 4, 2, 8, 9, 6, 1, 4, 7, 7, 8, 9, 7, 5, 1, 6, 9, 4,
7, 6)
length(Caseros)
length(SantosLugares)
length(PabloPodesta)
boxplot(Caseros ,SantosLugares,PabloPodesta, main="Asistencia a clases", names=c("Caseros", "SantosLugares", "PabloPodesta"))
summary(Caseros)
summary(x)
summary(Caseros)
summary(SantosLugares)
summary(PabloPodesta)
boxplot(Caseros ,SantosLugares,PabloPodesta, main="Asistencia a clases", names=c("Caseros", "SantosLugares", "PabloPodesta"))
apply(Caseros,range)
apply(Caseros,2,range)
range(Caseros)
summary(Caseros)
summary(SantosLugares)
summary(PabloPodesta)
range(Caseros)
range(SantosLugares)
range(PabloPodesta)
var(Caseros)
var(SantosLugares)
var(PabloPodesta)
boxplot(Caseros ,SantosLugares,PabloPodesta, main="Asistencia a clases", names=c("Caseros", "SantosLugares", "PabloPodesta"))
aov.1<-aov(x ~ f)
x <- c(Caseros,SantosLugares,PabloPodesta)
f <- factor(c(rep("Caseros",40),rep("SantosLugares",40),rep("PabloPodesta",40)))
aov.1<-aov(x ~ f)
print(summary(aov.1))
summary(aov.1)
tukey1<-TukeyHSD(aov.1)
plot(tukey1)
print(TukeyHSD(aov.1))
install.packages("forecast")
data(nottem)
nottem <- ts(nottem,start=c(1920,1),frequency=12)
start(nottem)
end(nottem)
frequency(nottem)
summary(nottem)
data(nottem)
force(nottem)
nottem <- ts(nottem,start=c(1920,1),frequency=12)
start(nottem)
end(nottem)
frequency(nottem)
summary(nottem)
head(nottem,1)
tail(nottem,1)
plot(nottem, col = "magenta")
plot(nottem, col = "magenta",main="Evolución de temperaturas en Nottinham",y="Temperatura (°F)")
plot(nottem, col = "magenta",main="Evolución de temperaturas en Nottinham",ylabel="Temperatura (°F)")
plot(nottem, col = "magenta",main="Evolución de temperaturas en Nottinham",ylab="Temperatura (°F)")
acf(nottem)
gglagplot(nottem.ts)
library(ggplot2)
gglagplot(nottem.ts)
library(ggplot2)
gglagplot(nottem.ts)
gglagplot(nottem)
library(forecast)
gglagplot(nottem)
gglagplot(nottem.ts)
boxplot(nottem~cycle(nottem),col ="blue")
cycle(nottem)
boxplot(nottem~cycle(nottem),col ="light blue", main="Boxplot de Temperaturas por mes",ylab="Temperatura (°F)",xlab="Mes")
acf(nottem)
install.packages('astsa')
require(astsa)
sarima(nottem, p = 1, d = 0, q = 0)
ggseasonplot(nottem, polar = TRUE)
gglagplot(nottem)
gglagplot(nottem, main="Lagplot de Temperaturas para Nottinham")
ggseasonplot(nottem, polar = TRUE)
sarima(nottem, p = 1, d = 0, q = 0)
plot(decompose(nottem))
AP<- ts(nottem,frequency = 12, start = c(1920,1), end = c(1938,12))
modelo1 <- auto.arima(AP, stationary =FALSE, seasonal = TRUE)
summary(modelo1)
ajuste<- forecast(modelo1, h = 12)
ajuste
ajuste$mean
plot(ajuste)
library(ggplot2)
SF<- window(nottem,start=c(1939,1),end=c(1939,12))
SF
SFAjuste = cbind(SF,ajuste$mean)
SFAjuste
(1/12)* sum((ajuste$mean - SF)^2)
ggplot(nottem, aes(y = nottem, x=time(nottem)))+geom_line()+geom_smooth(se=FALSE)
SF
SFAjuste
(1/12)* sum((ajuste$mean - SF)^2)
install.packages("Matrix")
install.packages("Hmisc")
install.packages("rlist")
install.packages("yaml")
install.packages("parallel")
install.packages("parallel")
install.packages("primes")
install.packages("bit64")
install.packages("IRdisplay")
install.packages("repr")
install.packages("vioplot")
install.packages("DT")
install.packages("ROCR")
install.packages("R.utils")
install.packages("Rcpp")
install.packages("devtools")
install.packages("ggplot2")
install.packages("gganimate")
install.packages("transformr")
install.packages("DiagrammeR")
install.packages("data.table")
install.packages("rpart")
install.packages("rpart.plot")
install.packages("treeClust")
install.packages("ranger")
install.packages("randomForest")
install.packages("xgboost")
install.packages("lightgbm")
install.packages("DiceKriging")
install.packages("mlrMBO")
install.packages("randomForest")
install.packages(c('repr', 'IRdisplay', 'evaluate', 'crayon', 'pbdZMQ', 'devtools', 'uuid', 'digest'))
install.packages('IRkernel')
library( "IRkernel" )
IRkernel::installspec()
quit()
install.packages("randomForest") #No me dejo
install.packages("randomForest") #No me dejo
install.packages("randomForest")
install.packages("Matrix")
install.packages("Hmisc")
install.packages("rlist")
install.packages("yaml")
install.packages("parallel")
install.packages("primes")
install.packages("bit64")
install.packages("IRdisplay")
install.packages("repr")
install.packages("vioplot")
install.packages("DT")
install.packages("ROCR")
install.packages("R.utils")
install.packages("Rcpp")
install.packages("devtools")
install.packages("ggplot2")
install.packages("gganimate")
install.packages("transformr")
install.packages("DiagrammeR")
install.packages("data.table")
install.packages("rpart")
install.packages("rpart.plot")
install.packages("treeClust")
install.packages("ranger")
install.packages("randomForest") #No me dejo
install.packages("xgboost")
install.packages("lightgbm")
install.packages("DiceKriging")
install.packages("mlrMBO")
install.packages(c('repr', 'IRdisplay', 'evaluate', 'crayon', 'pbdZMQ', 'devtools', 'uuid', 'digest'))
install.packages('IRkernel')
library( "IRkernel" )
IRkernel::installspec()
install.packages("randomForest") #No me dejo
quit()
rm( list=ls() )  #Borro todos los objetos
gc()   #Garbage Collection
require("data.table")
require("rpart")
require("parallel")
ksemillas  <- c(111599, 111611, 111623, 111637, 111641) #reemplazar por las propias semillas
particionar  <- function( data,  division, agrupa="",  campo="fold", start=1, seed=NA )
{
if( !is.na(seed) )   set.seed( seed )
bloque  <- unlist( mapply(  function(x,y) { rep( y, x )} ,   division,  seq( from=start, length.out=length(division) )  ) )
data[ , (campo) :=  sample( rep( bloque, ceiling(.N/length(bloque))) )[1:.N],
by= agrupa ]
}
ArbolEstimarGanancia  <- function( semilla, param_basicos )
{
#particiono estratificadamente el dataset
particionar( dataset, division=c(75,25), agrupa="clase_ternaria", seed= semilla )  #Cambiar por la primer semilla de cada uno !
#genero el modelo
modelo  <- rpart("clase_ternaria ~ .",     #quiero predecir clase_ternaria a partir del resto
data= dataset[ fold==1],  #fold==1  es training,  el 75% de los datos
xval= 0,
control= param_basicos )  #aqui van los parametros del arbol
#aplico el modelo a los datos de testing
prediccion  <- predict( modelo,   #el modelo que genere recien
dataset[ fold==2],  #fold==2  es testing, el 25% de los datos
type= "prob") #type= "prob"  es que devuelva la probabilidad
ArbolEstimarGanancia  <- function( semilla, param_basicos )
{
#particiono estratificadamente el dataset
particionar( dataset, division=c(75,25), agrupa="clase_ternaria", seed= semilla )  #Cambiar por la primer semilla de cada uno !
#genero el modelo
modelo  <- rpart("clase_ternaria ~ .",     #quiero predecir clase_ternaria a partir del resto
data= dataset[ fold==1],  #fold==1  es training,  el 75% de los datos
xval= 0,
control= param_basicos )  #aqui van los parametros del arbol
#aplico el modelo a los datos de testing
prediccion  <- predict( modelo,   #el modelo que genere recien
dataset[ fold==2],  #fold==2  es testing, el 25% de los datos
type= "prob") #type= "prob"  es que devuelva la probabilidad
#prediccion es una matriz con TRES columnas, llamadas "BAJA+1", "BAJA+2"  y "CONTINUA"
#cada columna es el vector de probabilidades
#calculo la ganancia en testing  qu es fold==2
ganancia_test  <- dataset[ fold==2,
sum( ifelse( prediccion[, "BAJA+2"]  >  1/60,
ifelse( clase_ternaria=="BAJA+2", 59000, -1000 ),
0 ) )]
#escalo la ganancia como si fuera todo el dataset
ganancia_test_normalizada  <-  ganancia_test / 0.3
return( ganancia_test_normalizada )
}
ArbolesMontecarlo  <- function( semillas, param_basicos )
{
#la funcion mcmapply  llama a la funcion ArbolEstimarGanancia  tantas veces como valores tenga el vector  ksemillas
ganancias  <- mcmapply( ArbolEstimarGanancia,
semillas,   #paso el vector de semillas, que debe ser el primer parametro de la funcion ArbolEstimarGanancia
MoreArgs= list( param_basicos),  #aqui paso el segundo parametro
SIMPLIFY= FALSE,
mc.cores= 1 )  #se puede subir a 5 si posee Linux o Mac OS
ganancia_promedio  <- mean( unlist(ganancias) )
return( ganancia_promedio )
}
setwd("C:\\Users\\HP\\Desktop\\ECM\\MDD")   #Establezco el Working Directory
dataset  <- fread("./datasets/paquete_premium_202011.csv")
dir.create( "./labo/exp/",  showWarnings = FALSE )
dir.create( "./labo/exp/HT2020/", showWarnings = FALSE )
archivo_salida  <- "./labo/exp/HT2020/gridsearch.txt"
Sys.time()
print(Sys.time())
Sys.time()
Sys.time()
a <- Sys.time()
require("data.table")
rm( list=ls() )  #Borro todos los objetos
gc()   #Garbage Collection
rm( list=ls() )  #Borro todos los objetos
gc()   #Garbage Collection
rm( list=ls() )  #Borro todos los objetos
gc()   #Garbage Collection
#Optimizacion Bayesiana de hiperparametros de  rpart
#limpio la memoria
rm( list=ls() )  #remove all objects
gc()             #garbage collection
require("data.table")
require("rlist")
require("rpart")
require("parallel")
#paquetes necesarios para la Bayesian Optimization
require("DiceKriging")
require("mlrMBO")
#Defino la  Optimizacion Bayesiana
kBO_iter  <- 100   #cantidad de iteraciones de la Optimizacion Bayesiana
hs  <- makeParamSet(
makeNumericParam("cp"       , lower= -1   , upper=    0.1),
makeIntegerParam("minsplit" , lower=  1L  , upper= 8000L),  #la letra L al final significa ENTERO
makeIntegerParam("minbucket", lower=  1L  , upper= 2000L),
makeIntegerParam("maxdepth" , lower=  3L  , upper=   20L),
forbidden = quote( minbucket > 0.5*minsplit ) )             # minbuket NO PUEDE ser mayor que la mitad de minsplit
ksemilla_azar  <- 111599   #cambiar por la primer semilla
#------------------------------------------------------------------------------
#graba a un archivo los componentes de lista
#para el primer registro, escribe antes los titulos
loguear  <- function( reg, arch=NA, folder="./work/", ext=".txt", verbose=TRUE )
{
archivo  <- arch
if( is.na(arch) )  archivo  <- paste0( folder, substitute( reg), ext )
if( !file.exists( archivo ) )  #Escribo los titulos
{
linea  <- paste0( "fecha\t",
paste( list.names(reg), collapse="\t" ), "\n" )
cat( linea, file=archivo )
}
linea  <- paste0( format(Sys.time(), "%Y%m%d %H%M%S"),  "\t",     #la fecha y hora
gsub( ", ", "\t", toString( reg ) ),  "\n" )
cat( linea, file=archivo, append=TRUE )  #grabo al archivo
if( verbose )  cat( linea )   #imprimo por pantalla
}
#------------------------------------------------------------------------------
#particionar agrega una columna llamada fold a un dataset que consiste en una particion estratificada segun agrupa
# particionar( data=dataset, division=c(70,30), agrupa=clase_ternaria, seed=semilla)   crea una particion 70, 30
# particionar( data=dataset, division=c(1,1,1,1,1), agrupa=clase_ternaria, seed=semilla)   divide el dataset en 5 particiones
particionar  <- function( data, division, agrupa="", campo="fold", start=1, seed=NA )
{
if( !is.na( seed)  )   set.seed( seed )
bloque  <- unlist( mapply(  function(x,y) { rep( y, x ) }, division, seq( from=start, length.out=length(division) )  ) )
data[ , (campo) :=  sample( rep( bloque, ceiling(.N/length(bloque))) )[1:.N],
by= agrupa ]
}
#------------------------------------------------------------------------------
#fold_test  tiene el numero de fold que voy a usar para testear, entreno en el resto de los folds
#param tiene los hiperparametros del arbol
ArbolSimple  <- function( fold_test, data, param )
{
#genero el modelo
modelo  <- rpart("clase_ternaria ~ .",
data= data[ fold != fold_test, ],  #entreno en todo MENOS el fold_test que uso para testing
xval= 0,
control= param )
#aplico el modelo a los datos de testing
prediccion  <- predict( modelo,
data[ fold==fold_test, ],  #aplico el modelo sobre los datos de testing
type= "prob")   #quiero que me devuelva probabilidades
prob_baja2  <- prediccion[, "BAJA+2"]  #esta es la probabilidad de baja
#calculo la ganancia
ganancia_testing  <- data[ fold==fold_test ][ prob_baja2 > 1/60,
sum( ifelse( clase_ternaria=="BAJA+2", 59000, -1000 ) )]
return( ganancia_testing )  #esta es la ganancia sobre el fold de testing, NO esta normalizada
}
#------------------------------------------------------------------------------
ArbolesCrossValidation  <- function( data, param, qfolds, pagrupa, semilla )
{
divi  <- rep( 1, qfolds )  # generalmente  c(1, 1, 1, 1, 1 )  cinco unos
particionar( data, divi, seed=semilla, agrupa=pagrupa )  #particiono en dataset en folds
ganancias  <- mcmapply( ArbolSimple,
seq(qfolds), # 1 2 3 4 5
MoreArgs= list( data, param),
SIMPLIFY= FALSE,
mc.cores= 1 )   #se puede subir a qfolds si posee Linux o Mac OS
data[ , fold := NULL ]
#devuelvo la primer ganancia y el promedio
ganancia_promedio  <- mean( unlist( ganancias ) )   #promedio las ganancias
ganancia_promedio_normalizada  <- ganancia_promedio * qfolds  #aqui normalizo la ganancia
return( ganancia_promedio_normalizada )
}
#------------------------------------------------------------------------------
#esta funcion solo puede recibir los parametros que se estan optimizando
#el resto de los parametros, lamentablemente se pasan como variables globales
EstimarGanancia  <- function( x )
{
GLOBAL_iteracion  <<-  GLOBAL_iteracion + 1
xval_folds  <- 5
ganancia  <- ArbolesCrossValidation( dataset,
param= x, #los hiperparametros del arbol
qfolds= xval_folds,  #la cantidad de folds
pagrupa= "clase_ternaria",
semilla= ksemilla_azar )
#logueo
xx  <- x
xx$xval_folds  <-  xval_folds
xx$ganancia  <- ganancia
xx$iteracion <- GLOBAL_iteracion
loguear( xx,  arch= archivo_log )
return( ganancia )
}
#------------------------------------------------------------------------------
#Aqui empieza el programa
setwd( "C:/Users/HP/Desktop/ECD/MDD" )
#cargo el dataset
dataset  <- fread("./datasets/paquete_premium_202011.csv")   #donde entreno
#creo la carpeta donde va el experimento
# HT  representa  Hiperparameter Tuning
dir.create( "./labo/exp/",  showWarnings = FALSE )
dir.create( "./labo/exp/HT3210/", showWarnings = FALSE )
setwd("C:/Users/HP/Desktop/ECD/MDD/labo/exp/HT3210")   #Establezco el Working Directory DEL EXPERIMENTO
archivo_log  <- "HT321.txt"
archivo_BO   <- "HT321.RDATA"
#leo si ya existe el log, para retomar en caso que se se corte el programa
GLOBAL_iteracion  <- 0
if( file.exists(archivo_log) )
{
tabla_log  <- fread( archivo_log )
GLOBAL_iteracion  <- nrow( tabla_log )
}
#Aqui comienza la configuracion de la Bayesian Optimization
funcion_optimizar  <- EstimarGanancia
configureMlr( show.learner.output= FALSE)
#configuro la busqueda bayesiana,  los hiperparametros que se van a optimizar
#por favor, no desesperarse por lo complejo
obj.fun  <- makeSingleObjectiveFunction(
fn=       funcion_optimizar,
minimize= FALSE,   #estoy Maximizando la ganancia
noisy=    TRUE,
par.set=  hs,
has.simple.signature = FALSE
)
ctrl  <- makeMBOControl( save.on.disk.at.time= 600,  save.file.path= archivo_BO)
ctrl  <- setMBOControlTermination(ctrl, iters= kBO_iter )
ctrl  <- setMBOControlInfill(ctrl, crit= makeMBOInfillCritEI())
surr.km  <- makeLearner("regr.km", predict.type= "se", covtype= "matern3_2", control= list(trace= TRUE))
#inicio la optimizacion bayesiana
if( !file.exists( archivo_BO ) ) {
run  <- mbo(obj.fun, learner = surr.km, control = ctrl)
} else  run  <- mboContinue( archivo_BO )   #retomo en caso que ya exista
